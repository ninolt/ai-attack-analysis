{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting cyberattacks - HAI 20.07 dataset\n",
    "\n",
    "This Jupyter notebook explores various approaches to implementing machine learning algorithms for detecting attacks in the HAI 20.07 dataset.\n",
    "\n",
    "Technical details : [here](https://github.com/icsdataset/hai/blob/master/hai_dataset_technical_details.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and normalizing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 18:44:41.308593: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import collections\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip_system_certs in ./.venv/lib/python3.11/site-packages (5.3)\n",
      "Requirement already satisfied: pip>=24.2 in ./.venv/lib/python3.11/site-packages (from pip_system_certs) (25.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pip_system_certs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Injecting system SSL certificates\n",
    "import pip_system_certs.wrapt_requests\n",
    "pip_system_certs.wrapt_requests.inject_truststore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use a dedicated validation dataset derived from the training dataset (and not from the testing dataset) because the 'normal' data in the testing dataset may be affected by previous attacks on the system, potentially resulting in a very inaccurate validation loss score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "validation = []\n",
    "test = []\n",
    "labels = []\n",
    "\n",
    "dataset_ids = [1, 2]\n",
    "\n",
    "for dataset_id in dataset_ids:\n",
    "    train_data, validation_data = train_test_split(\n",
    "        pd.read_csv(f\"https://raw.githubusercontent.com/icsdataset/hai/master/hai-20.07/train{dataset_id}.csv.gz\", sep=\";\", index_col=\"time\", usecols=list(range(60))).values,\n",
    "        # test size => validation size in this case\n",
    "        test_size=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    train.append(train_data)\n",
    "    validation.append(validation_data)\n",
    "    \n",
    "    test_data = pd.read_csv(f\"https://raw.githubusercontent.com/icsdataset/hai/master/hai-20.07/test{dataset_id}.csv.gz\", sep=\";\", index_col=\"time\").values\n",
    "\n",
    "    test.append(test_data[:, 0:-4])\n",
    "    labels.append(test_data[:, -4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data on [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 18:45:15.222830: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-10-25 18:45:15.224219: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 131518080 exceeds 10% of free system memory.\n",
      "2025-10-25 18:45:15.363301: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 65759040 exceeds 10% of free system memory.\n",
      "2025-10-25 18:45:15.467092: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 137635200 exceeds 10% of free system memory.\n",
      "2025-10-25 18:45:15.589064: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 68817600 exceeds 10% of free system memory.\n",
      "2025-10-25 18:45:15.920431: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 102461760 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataset_ids)):\n",
    "    reduced_min = np.min(train[i], axis=0)\n",
    "    reduced_max = np.max(train[i], axis=0)\n",
    "\n",
    "    asset_range = reduced_max - reduced_min\n",
    "\n",
    "    # Normalizing while making sure there is no divison by zero\n",
    "    train[i] = np.divide((train[i] - reduced_min), asset_range, out=np.zeros_like(train[i]), where=asset_range != 0)\n",
    "    test[i] = np.divide((test[i] - reduced_min), asset_range, out=np.zeros_like(test[i]), where=asset_range != 0)\n",
    "    validation[i] = np.divide((validation[i] - reduced_min), asset_range, out=np.zeros_like(validation[i]), where=asset_range != 0)\n",
    "    \n",
    "    train[i] = tf.cast(train[i], tf.float32)\n",
    "    test[i] = tf.cast(test[i], tf.float32)\n",
    "    validation[i] = tf.cast(validation[i], tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an anomaly detection model\n",
    "\n",
    "The goal here is to measure, at a given time and for a given sensor, how abnormal its value is compared to its normal behavior.\n",
    "\n",
    "To achieve this, we will use an autoencoder model. The concept is straightforward: we train the model on normal training data, and the model attempts to reproduce this normal data. The hypothesis is that the model will perform poorly when presented with abnormal data, such as during an attack on the system.\n",
    "\n",
    "In this case, we will build multiple models to gain a better understanding of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model n°1, Dataset n°1\n",
      "Epoch 1/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0900 - val_loss: 0.0511\n",
      "Epoch 2/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0464 - val_loss: 0.0436\n",
      "Epoch 3/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0408 - val_loss: 0.0392\n",
      "Epoch 4/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0379 - val_loss: 0.0370\n",
      "Epoch 5/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0355 - val_loss: 0.0340\n",
      "Epoch 6/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0328 - val_loss: 0.0322\n",
      "Epoch 7/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0318 - val_loss: 0.0313\n",
      "Epoch 8/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0310 - val_loss: 0.0307\n",
      "Epoch 9/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0305 - val_loss: 0.0304\n",
      "Epoch 10/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0302 - val_loss: 0.0300\n",
      "Model n°1, Dataset n°2\n",
      "Epoch 1/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1054 - val_loss: 0.0539\n",
      "Epoch 2/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0476 - val_loss: 0.0426\n",
      "Epoch 3/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0379 - val_loss: 0.0346\n",
      "Epoch 4/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0340 - val_loss: 0.0334\n",
      "Epoch 5/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0324 - val_loss: 0.0310\n",
      "Epoch 6/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0294 - val_loss: 0.0279\n",
      "Epoch 7/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0273 - val_loss: 0.0270\n",
      "Epoch 8/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0267 - val_loss: 0.0267\n",
      "Epoch 9/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0265 - val_loss: 0.0266\n",
      "Epoch 10/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0264 - val_loss: 0.0264\n",
      "Model n°2, Dataset n°1\n",
      "Epoch 1/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0933 - val_loss: 0.0524\n",
      "Epoch 2/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0440 - val_loss: 0.0396\n",
      "Epoch 3/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0365 - val_loss: 0.0339\n",
      "Epoch 4/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0324 - val_loss: 0.0311\n",
      "Epoch 5/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0301 - val_loss: 0.0292\n",
      "Epoch 6/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0286 - val_loss: 0.0282\n",
      "Epoch 7/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0275 - val_loss: 0.0271\n",
      "Epoch 8/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0265 - val_loss: 0.0261\n",
      "Epoch 9/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0257 - val_loss: 0.0255\n",
      "Epoch 10/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0251 - val_loss: 0.0249\n",
      "Model n°2, Dataset n°2\n",
      "Epoch 1/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.1139 - val_loss: 0.0569\n",
      "Epoch 2/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0497 - val_loss: 0.0447\n",
      "Epoch 3/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0402 - val_loss: 0.0361\n",
      "Epoch 4/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0352 - val_loss: 0.0347\n",
      "Epoch 5/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0344 - val_loss: 0.0341\n",
      "Epoch 6/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0340 - val_loss: 0.0338\n",
      "Epoch 7/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0337 - val_loss: 0.0335\n",
      "Epoch 8/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0334 - val_loss: 0.0333\n",
      "Epoch 9/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0331 - val_loss: 0.0331\n",
      "Epoch 10/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0325 - val_loss: 0.0318\n",
      "Model n°3, Dataset n°1\n",
      "Epoch 1/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0944 - val_loss: 0.0596\n",
      "Epoch 2/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0494 - val_loss: 0.0451\n",
      "Epoch 3/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0433 - val_loss: 0.0421\n",
      "Epoch 4/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.0403 - val_loss: 0.0389\n",
      "Epoch 5/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0377 - val_loss: 0.0372\n",
      "Epoch 6/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0367 - val_loss: 0.0365\n",
      "Epoch 7/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0363 - val_loss: 0.0362\n",
      "Epoch 8/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0360 - val_loss: 0.0360\n",
      "Epoch 9/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0349 - val_loss: 0.0350\n",
      "Epoch 10/10\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0346 - val_loss: 0.0347\n",
      "Model n°3, Dataset n°2\n",
      "Epoch 1/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1015 - val_loss: 0.0484\n",
      "Epoch 2/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0413 - val_loss: 0.0370\n",
      "Epoch 3/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0352 - val_loss: 0.0336\n",
      "Epoch 4/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0324 - val_loss: 0.0314\n",
      "Epoch 5/10\n",
      "\u001b[1m  1/424\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - loss: 0.0310"
     ]
    }
   ],
   "source": [
    "class AttackDetectionModel(Model):\n",
    "    def __init__(self, layers_sizes):\n",
    "        super(AttackDetectionModel, self).__init__()\n",
    "\n",
    "        # Layers sizes : [Input/Ouput size, hidden layer, Bottleneck size]\n",
    "        # Only one hidden layer because of the small number of sensors\n",
    "        \n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Dense(layers_sizes[1], activation=\"relu\"), # Encoder hidden layer\n",
    "            layers.Dense(layers_sizes[2], activation=\"relu\")  # Bottleneck (lower-dimensionnal representation)\n",
    "        ])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Dense(layers_sizes[1], activation=\"relu\"), # Decoder hidden layer\n",
    "            layers.Dense(layers_sizes[0], activation=\"sigmoid\")  # Output of the model\n",
    "        ])\n",
    "\n",
    "    def call(self, sensors):\n",
    "        encoded = self.encoder(sensors)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "models = []\n",
    "# Compare the results from different models\n",
    "number_models = 5\n",
    "\n",
    "# models => list of X groups of models => multiple models (two for the two datasets) => model + history\n",
    "\n",
    "for i in range(number_models):\n",
    "    models.append([])\n",
    "    # Each training dataset has different schedule routines, and thus needs to be trained on his own\n",
    "    for j in range(len(dataset_ids)):\n",
    "        print(\"Model n°\" + str(i+1) + \", Dataset n°\" + str(j+1))\n",
    "        models[i].append({})\n",
    "        models[i][j][\"model\"] = AttackDetectionModel([train[0].shape[1], 35, 20])\n",
    "        models[i][j][\"model\"].compile(optimizer='adam', loss='mae')\n",
    "        models[i][j][\"history\"] = models[i][j][\"model\"].fit(train[j], train[j],\n",
    "                  epochs=10,\n",
    "                  batch_size=512,\n",
    "                  validation_data=(validation[j], validation[j]),\n",
    "                  shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation loss of the first group of models as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset_ids)):\n",
    "    plt.plot(models[0][i][\"history\"].history[\"loss\"], label=\"Training Loss\")\n",
    "    plt.plot(models[0][i][\"history\"].history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting every reconstructions of the testing data from the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [[models[j][i][\"model\"].predict(test[i]) for i in range(len(dataset_ids))] for j in range(len(models))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the results given by the model\n",
    "\n",
    "For the rest of this section, we will be estimating the relationships between sensors during an attack. We will take the sensor n°XYZ as an example for out analysis.\n",
    "\n",
    "Here is a comparaison between the prediction of the model for the sensor (in red), and the actual input of the sensor being attacked (in blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.rot90(predictions[0][1])[sensor], color=\"red\", label=\"Reconstruction\")\n",
    "plt.plot(np.rot90(test[1])[sensor], color=\"blue\", label=\"Input\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Normalized value of the sensor\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for calculating the loss between the real testing data, and the reconstruction from a given model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss(dataset_id, iteration):\n",
    "    # Squared difference\n",
    "    return np.square(test[dataset_id] - predictions[iteration][dataset_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating all the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = [[test_loss(i, j) for i in range(len(dataset_ids))] for j in range(number_models)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the squared loss of the chosen sensors between the prediction and the sensor values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.rot90(loss[0][1])[sensor], color=\"red\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Squared loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly identify two periods when the prediction model failed to predict the sensor’s behavior. This likely means that these two periods correspond to times when the sensor was under attacked and misbehaved.\n",
    "\n",
    "In the next step, we are separating each attack from the losses. This is accomplished using the testing labels, which indicate when the system detected an attack.\n",
    "\n",
    "Additionally, we are adding padding before and after each attack to avoid losing information.\n",
    "\n",
    "`attack (list of sensor values over time) = buffer + labels indicating attack + buffer (from the loss calculation above)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the ids of the different attacks (cf. detailed pdf of the dataset)\n",
    "attack_ids = [\"A\" + j + str(i).zfill(2) for j in [\"1\", \"2\"] for i in range(1, 20)]\n",
    "\n",
    "buffered_attacks = {}\n",
    "\n",
    "start_buffer = 20\n",
    "end_buffer = 200\n",
    "\n",
    "for j in range(number_models):\n",
    "    attack_iterator = iter(attack_ids)\n",
    "    reading_attack = False\n",
    "    \n",
    "    for dataset_id, dataset in enumerate(labels):\n",
    "        for i, label_sample in enumerate(dataset):\n",
    "            if label_sample[0] == 1 and not reading_attack:\n",
    "                reading_attack = True\n",
    "                attack_start = i\n",
    "            elif label_sample[0] == 0 and reading_attack:\n",
    "                reading_attack = False\n",
    "                attack = next(attack_iterator)\n",
    "\n",
    "                buffered_attacks[attack] = buffered_attacks.get(attack, []) + [loss[j][dataset_id][attack_start - start_buffer:i + end_buffer]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is giving an indicative threshold for \"abnormal loss\" for a given vector on a timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(vector):\n",
    "    return np.mean(vector) + np.std(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to transform each value of the vector to boolean of (above threshold or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossing_threshold(vector):\n",
    "    return vector - threshold(vector) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is deciding if a sensor is abnormal on a given timeline (returning boolean), depending on two criteria :\n",
    "- The sensor shouldn't be abnormal before the attack (we can argue that the values of the sensor cannot really be trusted)\n",
    "- The sensor should reach a minimum of abnormal \"quantity\" during the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input : Vector of boolean \"is above threshold for a given timestamp\"\n",
    "def is_sensor_affected(vector):\n",
    "    # Criteria 1 : Not already abnormal before the attack\n",
    "    # Criteria 2 : Reach abnormal level during the attack (or during the buffer time at the end)\n",
    "    min_percentage_abn_duration = 0.15\n",
    "    \n",
    "    return (np.sum(vector[:start_buffer]) == 0) and (np.sum(vector[start_buffer:]) / len(vector[start_buffer:]) >= min_percentage_abn_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we maintain the same shape of `(buffer + length of labels indicating attack + buffer) x sensor_amount`, but we modify the content.\n",
    "\n",
    "For a given attack:\n",
    "- If a sensor is considered 'affected' (as determined by the function mentioned above), each of its values will be converted to booleans, indicating whether they are 'abnormal' or not.\n",
    "- If the sensor is not considered affected, it will be filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks_sensors = {}\n",
    "\n",
    "for i in range(number_models):\n",
    "    for attack_id, sensors_list in buffered_attacks.items():\n",
    "        crossing_sensors = np.apply_along_axis(crossing_threshold, 0, sensors_list[i])\n",
    "        affected_sensors = np.apply_along_axis(is_sensor_affected, 0, crossing_sensors)\n",
    "\n",
    "        attacks_sensors[attack_id] = attacks_sensors.get(attack_id, []) + [np.empty(crossing_sensors.shape)]\n",
    "        \n",
    "        for column_id in range(crossing_sensors.shape[1]):\n",
    "            if affected_sensors[column_id]:\n",
    "                attacks_sensors[attack_id][i][:, column_id] = crossing_sensors[:, column_id]\n",
    "            else:\n",
    "                # Filling the sensor values with zeros instead of removing it in order to keep the index of each sensor correct\n",
    "                attacks_sensors[attack_id][i][:, column_id] = np.zeros((crossing_sensors.shape[0],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph below shows the behavior of the selected sensor during a specific attack period. If the sensor’s loss value at a given time exceeds the `mean + standard deviation`, the sensor’s value at that time is considered abnormal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = \"A217\"\n",
    "\n",
    "plt.plot(np.rot90(attacks_sensors[attack][0])[sensor], color=\"blue\")\n",
    "plt.title(\"Behaviour of the sensor \" + str(sensor) + \" for the attack \" + attack)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Is the sensor misbehaving\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code is filling with zeros the \"small peeks\" of abnormal timestamps. If they are not considered big enough, they are considered not meaningfull enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum size (in seconds) of an \"abnormal peek\" during an attack for it to be considered \"meaningfull\"\n",
    "min_peek_lenght = 10\n",
    "\n",
    "for attack_id, sensors_list in attacks_sensors.items():\n",
    "    for j in range(number_models):\n",
    "        for sensor_id, sensor in enumerate(sensors_list[j].T):\n",
    "            reading_peek = False\n",
    "            \n",
    "            for i, value in enumerate(sensor):\n",
    "                if value and not reading_peek:\n",
    "                    reading_peek = True\n",
    "                    start_peek = i\n",
    "                elif not value and reading_peek:\n",
    "                    reading_peek = False\n",
    "    \n",
    "                    # Replace small peeks with zeros\n",
    "                    if i - start_peek < min_peek_lenght:\n",
    "                        attacks_sensors[attack_id][j][start_peek:i, sensor_id].fill(0)\n",
    "    \n",
    "            if reading_peek and len(sensor) - start_peek < min_peek_lenght:\n",
    "                attacks_sensors[attack_id][j][start_peek:len(sensor), sensor_id].fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the correlation score between two sensors. The shift window represents the amount of time (in seconds) allowed for shifting the two signals. In other words, we permit a delay of X seconds (in both directions) to achieve the best possible correlation score. However, this shift is not retained later; it is merely a temporary adjustment to avoid being overly strict in the correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gap of time for taking into account \"delayed impacts\" between two sensors\n",
    "shift_window = 5\n",
    "\n",
    "# Function to determine if two sensors are influencing each other\n",
    "def correlation(sensor_1, sensor_2):\n",
    "    # Adding time padding\n",
    "    sensor_1 = np.append(np.zeros((shift_window)), sensor_1)\n",
    "    sensor_1 = np.append(sensor_1, np.zeros((shift_window)))\n",
    "\n",
    "    return np.correlate(sensor_1, sensor_2, mode=\"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calculates the correlation score between every possible pair of sensors. If the score is not sufficiently high, the pair of sensors for the given attack will not be considered.\n",
    "\n",
    "Next, we will identify the first timestamp where both sensors exhibit abnormal behavior simultaneously.\n",
    "\n",
    "Finally, we will examine the timestamp immediately before the 'first meeting timestamp.' If one sensor is abnormal and the other is not, we assume that the first sensor is 'affecting' the second one. If both sensors become abnormal at the same time, we assume that they influence each other.\n",
    "\n",
    "Reading: (1, 2) = sensor 1 is affecting sensor 2\n",
    "\n",
    "If both sensors are affecting each other, two relations, (1, 2) and (2, 1), will be added to the list.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_chains = {}\n",
    "\n",
    "relationship_min_value = 20\n",
    "\n",
    "for attack_id, sensors_list in attacks_sensors.items():\n",
    "    attack_chains[attack_id] = []\n",
    "\n",
    "    for i, sensors in enumerate(sensors_list):\n",
    "        attack_chains[attack_id].append([])\n",
    "        \n",
    "        for sensor_1 in range(sensors.shape[1] - 1):\n",
    "            for sensor_2 in range(sensor_1 + 1, sensors.shape[1]):\n",
    "                # If the two sensors are closely correlated\n",
    "                if np.max(correlation(sensors[:, sensor_1], sensors[:, sensor_2])) >= relationship_min_value:\n",
    "                    first_meeting_timestamp = None\n",
    "                    # First timestamp where both sensors are abnormal\n",
    "                    for timestamp in range(sensors.shape[0]):\n",
    "                        if sensors[timestamp, sensor_1] and sensors[timestamp, sensor_2]:\n",
    "                            first_meeting_timestamp = timestamp\n",
    "                            break\n",
    "    \n",
    "                    # NOTE : The case were the two sensors are correlated (with a shift) but in real life never abnormal\n",
    "                    # at the same time never came by, so it is not taken into account here\n",
    "                    \n",
    "                    # If there is a meeting timestamp\n",
    "                    if first_meeting_timestamp != None:\n",
    "                        # If the two sensors started being abnormal at the same time\n",
    "                        if first_meeting_timestamp == 0 or (not sensors[first_meeting_timestamp-1, sensor_1] and not sensors[first_meeting_timestamp-1, sensor_2]):\n",
    "                            # Then both sensors are influencing each other\n",
    "                            attack_chains[attack_id][i] += [(sensor_1, sensor_2), (sensor_2, sensor_1)]\n",
    "    \n",
    "                        # If the first sensor was abnormal right before first meeting point\n",
    "                        elif sensors[first_meeting_timestamp-1, sensor_1]:\n",
    "                            # Then the first sensor is influencing the second one\n",
    "                            attack_chains[attack_id][i].append((sensor_1, sensor_2))\n",
    "    \n",
    "                        # If the second sensor was abnormal right before first meeting point\n",
    "                        elif sensors[first_meeting_timestamp-1, sensor_2]:\n",
    "                            # Then the second sensor is influencing the first one\n",
    "                            attack_chains[attack_id][i].append((sensor_2, sensor_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we finally utilize the multiple models: since a single model might be incorrect in some edge cases, we cannot fully rely on it to determine the relationships we need.\n",
    "\n",
    "To address this issue, we previously decided to train multiple identical models. This allows us to observe recurrent behaviors across the models and identify which decisions appear to be edge cases.\n",
    "\n",
    "Here, we are evaluating how frequently a relationship between two sensors is detected by the different models.\n",
    "\n",
    "If fewer than half of the models detect a relationship, it is considered irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_relations_amount = 0\n",
    "unkept_relations_amount = 0\n",
    "\n",
    "seen = {}\n",
    "kept_relations = {}\n",
    "\n",
    "for attack_id, chains in attack_chains.items():\n",
    "    seen[attack_id] = []\n",
    "    kept_relations[attack_id] = []\n",
    "    for i, couples in enumerate(chains):\n",
    "        for couple in couples:\n",
    "            if couple not in seen[attack_id]:\n",
    "                occurences = 1\n",
    "\n",
    "                for j, other_chain in enumerate(chains):\n",
    "                    if i != j and couple in other_chain:\n",
    "                        occurences += 1\n",
    "                            \n",
    "                if occurences >= number_models * 0.5:\n",
    "                    kept_relations_amount += 1\n",
    "                    kept_relations[attack_id].append(couple)\n",
    "                else:\n",
    "                    unkept_relations_amount += 1\n",
    "                    \n",
    "                seen[attack_id].append(couple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Relations appearing accross half or more models :\", kept_relations_amount)\n",
    "print(\"Relations not appearing enough accross multiple models :\", unkept_relations_amount)\n",
    "print(\"\\n\".join([attack_id + \" : \" + str(couples) for attack_id, couples in kept_relations.items()]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
